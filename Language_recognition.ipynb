{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Language recognition.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO1w/coVfRKiwdK8euRpxrd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NavedAFZ/NLP/blob/master/Language_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHnMJZpSpGfA",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "#Detecting Text Language by Counting Stop Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPln_FARwwGK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk # https://www.nltk.org/install.html\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ue4H994_WXGX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = \"Yo man, it's time for you to shut yo' mouth! I ain't even messin' dawg.\"\n",
        "import sys\n",
        "\n",
        "try:\n",
        "    from nltk.tokenize import wordpunct_tokenize # RE-based tokenizer which splits text on whitespace and punctuation (except for underscore)\n",
        "except ImportError:\n",
        "    print('[!] You need to install nltk (http://nltk.org/index.html)')\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKSiwhJLtjXV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "outputId": "8a12e3b0-bd35-4a6d-e94e-175fa0711390"
      },
      "source": [
        "test_tokens = wordpunct_tokenize(text)\n",
        "test_tokens"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Yo',\n",
              " 'man',\n",
              " ',',\n",
              " 'it',\n",
              " \"'\",\n",
              " 's',\n",
              " 'time',\n",
              " 'for',\n",
              " 'you',\n",
              " 'to',\n",
              " 'shut',\n",
              " 'yo',\n",
              " \"'\",\n",
              " 'mouth',\n",
              " '!',\n",
              " 'I',\n",
              " 'ain',\n",
              " \"'\",\n",
              " 't',\n",
              " 'even',\n",
              " 'messin',\n",
              " \"'\",\n",
              " 'dawg',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXb1-eYztvH_",
        "colab_type": "text"
      },
      "source": [
        "There are other tokenizers e.g. RegexpTokenizer where you can enter your own regexp, WhitespaceTokenizer (similar to Python's string.split()) and BlanklineTokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kb9eHjyXt9yo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "039a71f0-4760-4810-afb4-70151ba505a5"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GSRImcjtkpV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "5ab8af86-9bcf-49b8-c175-2a6033b48d6d"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stopwords.readme().replace('\\n', ' ') # Since this is raw text, we need to replace \\n's with spaces for it to be readable."
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Stopwords Corpus  This corpus contains lists of stop words for several languages.  These are high-frequency grammatical words which are usually ignored in text retrieval applications.  They were obtained from: http://anoncvs.postgresql.org/cvsweb.cgi/pgsql/src/backend/snowball/stopwords/  The stop words for the Romanian language were obtained from: http://arlc.ro/resources/  The English list has been augmented https://github.com/nltk/nltk_data/issues/22  The German list has been corrected https://github.com/nltk/nltk_data/pull/49  A Kazakh list has been added https://github.com/nltk/nltk_data/pull/52  A Nepali list has been added https://github.com/nltk/nltk_data/pull/83  An Azerbaijani list has been added https://github.com/nltk/nltk_data/pull/100  A Greek list has been added https://github.com/nltk/nltk_data/pull/103  An Indonesian list has been added https://github.com/nltk/nltk_data/pull/112 '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ed9nxSZt6FB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "outputId": "ab3b11d0-649a-471b-8be5-38cd7b259858"
      },
      "source": [
        "stopwords.fileids() # Most corpora consist of a set of files, each containing a piece of text. A list of identifiers for these files is accessed via fileids()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['arabic',\n",
              " 'azerbaijani',\n",
              " 'danish',\n",
              " 'dutch',\n",
              " 'english',\n",
              " 'finnish',\n",
              " 'french',\n",
              " 'german',\n",
              " 'greek',\n",
              " 'hungarian',\n",
              " 'indonesian',\n",
              " 'italian',\n",
              " 'kazakh',\n",
              " 'nepali',\n",
              " 'norwegian',\n",
              " 'portuguese',\n",
              " 'romanian',\n",
              " 'russian',\n",
              " 'slovene',\n",
              " 'spanish',\n",
              " 'swedish',\n",
              " 'tajik',\n",
              " 'turkish']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1p1JccISuM6v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "8923687a-a75e-459e-9571-4e69f83bee17"
      },
      "source": [
        "stopwords.raw('arabic').replace('\\n', ' ') # Better"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'إذ إذا إذما إذن أف أقل أكثر ألا إلا التي الذي الذين اللاتي اللائي اللتان اللتيا اللتين اللذان اللذين اللواتي إلى إليك إليكم إليكما إليكن أم أما أما إما أن إن إنا أنا أنت أنتم أنتما أنتن إنما إنه أنى أنى آه آها أو أولاء أولئك أوه آي أي أيها إي أين أين أينما إيه بخ بس بعد بعض بك بكم بكم بكما بكن بل بلى بما بماذا بمن بنا به بها بهم بهما بهن بي بين بيد تلك تلكم تلكما ته تي تين تينك ثم ثمة حاشا حبذا حتى حيث حيثما حين خلا دون ذا ذات ذاك ذان ذانك ذلك ذلكم ذلكما ذلكن ذه ذو ذوا ذواتا ذواتي ذي ذين ذينك ريث سوف سوى شتان عدا عسى عل على عليك عليه عما عن عند غير فإذا فإن فلا فمن في فيم فيما فيه فيها قد كأن كأنما كأي كأين كذا كذلك كل كلا كلاهما كلتا كلما كليكما كليهما كم كم كما كي كيت كيف كيفما لا لاسيما لدى لست لستم لستما لستن لسن لسنا لعل لك لكم لكما لكن لكنما لكي لكيلا لم لما لن لنا له لها لهم لهما لهن لو لولا لوما لي لئن ليت ليس ليسا ليست ليستا ليسوا ما ماذا متى مذ مع مما ممن من منه منها منذ مه مهما نحن نحو نعم ها هاتان هاته هاتي هاتين هاك هاهنا هذا هذان هذه هذي هذين هكذا هل هلا هم هما هن هنا هناك هنالك هو هؤلاء هي هيا هيت هيهات والذي والذين وإذ وإذا وإن ولا ولكن ولو وما ومن وهو يا '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dujvJHU0uk1x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "97f53630-6f76-467d-cd66-cb931692a796"
      },
      "source": [
        "stopwords.words('english')[:10]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CySOFYeOw9Yk",
        "colab_type": "text"
      },
      "source": [
        "#Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3v26fXEjxDr9",
        "colab_type": "text"
      },
      "source": [
        "We loop through the list of stop words in all languages and check how many stop words our test text contains in each language. The text is then classified to be in the language in which it has the most stop words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWk7wYk4vJs6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "outputId": "0e08c274-5409-484a-a8d0-ac14cb7eebb7"
      },
      "source": [
        "language_ratios = {}\n",
        "\n",
        "test_words = [word.lower() for word in test_tokens] # lowercase all tokens\n",
        "test_words_set = set(test_words)\n",
        "\n",
        "for language in stopwords.fileids():\n",
        "    stopwords_set = set(stopwords.words(language)) # For some languages eg. Russian, it would be a wise idea to tokenize the stop words by punctuation too.\n",
        "    common_elements = test_words_set.intersection(stopwords_set)\n",
        "    language_ratios[language] = len(common_elements) # language \"score\"\n",
        "    \n",
        "language_ratios"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'arabic': 0,\n",
              " 'azerbaijani': 0,\n",
              " 'danish': 3,\n",
              " 'dutch': 0,\n",
              " 'english': 8,\n",
              " 'finnish': 0,\n",
              " 'french': 2,\n",
              " 'german': 1,\n",
              " 'greek': 0,\n",
              " 'hungarian': 1,\n",
              " 'indonesian': 0,\n",
              " 'italian': 1,\n",
              " 'kazakh': 0,\n",
              " 'nepali': 0,\n",
              " 'norwegian': 3,\n",
              " 'portuguese': 1,\n",
              " 'romanian': 2,\n",
              " 'russian': 0,\n",
              " 'slovene': 2,\n",
              " 'spanish': 1,\n",
              " 'swedish': 2,\n",
              " 'tajik': 0,\n",
              " 'turkish': 0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5s8elZq1xShN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_words_set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rl1B8cg5wXvm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0515c7fa-d03d-4d20-e4b4-e0c1f292ff7e"
      },
      "source": [
        "most_rated_language = max(language_ratios, key=language_ratios.get) # The key parameter to the max() function is a function that computes a key. In our case, we already have a key so we set key to languages_ratios.get which actually returns the key.\n",
        "most_rated_language"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'english'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0N1vT_rxo1e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6071739a-bc70-444b-8590-32370f558cd8"
      },
      "source": [
        "test_words_set.intersection(set(stopwords.words(most_rated_language))) # We can see which English stop words were found."
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ain', 'for', 'i', 'it', 's', 't', 'to', 'you'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RA15huN2yAwm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}